# ğŸ“± SignLensAI â€” Reconnaissance de la langue des signes (ASL)

Projet Ã©tudiant (introduction au Machine Learning / Deep Learning).  
Application mobile **Expo React Native** permettant de reconnaÃ®tre les signes de lâ€™alphabet amÃ©ricain (ASL) Ã  partir de la camÃ©ra du tÃ©lÃ©phone.

---

## ğŸš€ FonctionnalitÃ©s

- **Mode Photo** : importer une image ou prendre une photo pour obtenir la prÃ©diction (Top-3).
- **Mode Live** : lecture continue de la camÃ©ra â†’ affichage en temps rÃ©el des lettres reconnues, avec lissage anti-bruit.
- **Lecture vocale** : prononce la lettre reconnue (via `expo-speech`).
  
---

## ğŸ—ï¸ Architecture

- **Frontend mobile** : Expo React Native
  - Navigation : `@react-navigation/native`
  - CamÃ©ra : `expo-camera`
  - Reconnaissance : **API Flask**
  - UI : composants React Native custom

- **Backend** : API Flask
  - Expose `/predict_image` et `/predict_landmarks`
  - Charge le modÃ¨le **mediapipe_vector_model.h5**
  - Retourne la prÃ©diction 


---

## âš™ï¸ Installation & Lancement

### PrÃ©requis
- Node.js + npm
- Expo CLI (`npx expo`)
- Expo Go installÃ© sur smartphone

### Ã‰tapes

```bash
# 1. Cloner le repo
git clone https://github.com/Cloxylol/deepLearnigSignLanguage.git
cd signlensai

# 2. Installer les dÃ©pendances
npm install

# 3. Lancer Expo
npx expo start

```

Scanner le QR code avec Expo Go sur ton tÃ©lÃ©phone.



##  ğŸ‘¥ Auteurs

+ CloÃ© Petetin â€” frontend mobile (Expo)

+ RÃ©my Legras â€” backend Flask (API / modÃ¨le)





